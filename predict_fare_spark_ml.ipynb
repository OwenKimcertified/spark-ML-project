{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA FROM HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hdfs\n",
      "  Using cached hdfs-2.7.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docopt\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /home/owen/anaconda3/lib/python3.9/site-packages (from hdfs) (2.28.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/owen/anaconda3/lib/python3.9/site-packages (from hdfs) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/owen/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->hdfs) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/owen/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->hdfs) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/owen/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->hdfs) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/owen/anaconda3/lib/python3.9/site-packages (from requests>=2.7.0->hdfs) (2.0.4)\n",
      "Building wheels for collected packages: hdfs, docopt\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.2-py3-none-any.whl size=34171 sha256=a76e175ab3ed994b57be8ae6250bed7ccc6c1cd8f45aef53c83374bdac764f3c\n",
      "  Stored in directory: /home/owen/.cache/pip/wheels/92/ba/84/69be48a3f77fdb2d9c3f72e00e4e63a632dfd2c481ff49ecf3\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=8dc5c0976cc7a854808354f38a8ae9b3fd6a4e5b49131dcec74c10df21fcaa6c\n",
      "  Stored in directory: /home/owen/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built hdfs docopt\n",
      "Installing collected packages: docopt, hdfs\n",
      "Successfully installed docopt-0.6.2 hdfs-2.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "import pandas as pd\n",
    "\n",
    "hdfs_client = InsecureClient('http://127.0.0.1:9870', user='owen')\n",
    "\n",
    "hdfs_directory = '/eda_trip_df/'\n",
    "\n",
    "\n",
    "file_list = hdfs_client.list(hdfs_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part0.csv', 'part1.csv', 'part2.csv']\n"
     ]
    }
   ],
   "source": [
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK - ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "local_ip = '127.0.0.1'\n",
    "os.environ['SPARK_LOCAL_IP'] = local_ip\n",
    "\n",
    "MAX_MEMORY = '5g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/26 21:18:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/26 21:18:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "ss = SparkSession.builder.appName('spark-ml')\\\n",
    "                          .config('spark.executer.memory', MAX_MEMORY)\\\n",
    "                          .config('spark.driver.memory', MAX_MEMORY)\\\n",
    "                          .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
